{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7cdcfdd-a4ff-49f7-9dda-f9535fc61d8d",
   "metadata": {
    "id": "e7cdcfdd-a4ff-49f7-9dda-f9535fc61d8d"
   },
   "outputs": [],
   "source": [
    "# DSC670 - Week7 - Exercise  - Tuning and Adapting LLMs Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333992f3-eb86-4642-9705-4c9dda068ae9",
   "metadata": {
    "id": "333992f3-eb86-4642-9705-4c9dda068ae9"
   },
   "source": [
    "Techniques such as LoRA take considerable time and computational resources (including the need for a GPU). For this weekâ€™s assignment, re-build the model from the Matthew MacFarquhar article.\n",
    "\n",
    "https://blog.devgenius.io/sculpting-language-gpt-2-fine-tuning-with-lora-1caf3bfbc3c6\n",
    "\n",
    "\n",
    "Since a GPU is required, please consider using Google Colab. You can change the runtime type to GPU (T4). It doesnâ€™t take long to train the model.\n",
    "\n",
    "\n",
    "\n",
    "Build the solution using a Jupyter Notebook (no other Python is acceptable) and include Markdown that explains what the author is doing in the code. Iâ€™m looking for your commentary and evaluation in your own words.content)cise Code.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q3VUNwonOvdw",
   "metadata": {
    "id": "Q3VUNwonOvdw"
   },
   "source": [
    "## ------------------------------------------\n",
    "## Import necessary libraries\n",
    "## ------------------------------------------\n",
    "##  import torch\n",
    "\n",
    "Imports PyTorch, a deep learning framework used for:\n",
    "\n",
    "Building and training neural networks\n",
    "\n",
    "Handling tensors (multi-dimensional arrays)\n",
    "\n",
    "Running computations on GPUs or CPUs\n",
    "\n",
    "In this script, PyTorch is used for:\n",
    "\n",
    "Moving models and data to GPU (model.to(device))\n",
    "\n",
    "Saving model weights (torch.save(...))\n",
    "\n",
    "Disabling gradient computation (with torch.no_grad():)\n",
    "\n",
    "# from transformers import (...)\n",
    "\n",
    "This imports several key tools from Hugging Face Transformers, a popular library for working with pre-trained language models like GPT-2, BERT, etc.\n",
    "\n",
    "## AutoTokenizer\n",
    "\n",
    "Automatically downloads and loads the correct tokenizer for the model name (e.g., \"gpt2\").\n",
    "\n",
    "Tokenizers convert raw text into token IDs (numbers) that the model understands, and back into text.\n",
    "\n",
    "## AutoModelForCausalLM\n",
    "\n",
    "Automatically loads a pre-trained language model suitable for causal language modeling (i.e., predicting the next word).\n",
    "\n",
    "Example: GPT-2, GPT-Neo, etc.\n",
    "\n",
    "\"CausalLM\" stands for Causal Language Model, which is trained to generate text.\n",
    "\n",
    "## Trainer\n",
    "\n",
    "A high-level training utility from Hugging Face that simplifies model training.\n",
    "\n",
    "It handles gradient computation, batching, optimization, evaluation, and logging automatically.\n",
    "\n",
    "## TrainingArguments\n",
    "\n",
    "A configuration class that defines training hyperparameters, such as:\n",
    "\n",
    "batch_size, learning_rate, max_steps, etc.\n",
    "\n",
    "output_dir (where model checkpoints are saved)\n",
    "\n",
    "logging_steps (how often to print training progress)\n",
    "\n",
    "## DataCollatorForLanguageModeling\n",
    "\n",
    "A utility that prepares batches of tokenized text for language modeling.\n",
    "\n",
    "It ensures that input sequences are padded to equal length in each batch and optionally masks tokens for masked LM (not used here since GPT-2 is causal).\n",
    "\n",
    "## from peft import LoraConfig, get_peft_model\n",
    "\n",
    "Imports tools from the PEFT (Parameter-Efficient Fine-Tuning) library.\n",
    "\n",
    "PEFT allows you to fine-tune very large models (like GPT-2) without updating all parameters â€” only small adapter layers (LoRA layers) are trained.\n",
    "\n",
    "## LoraConfig\n",
    "\n",
    "Configuration object that defines LoRA settings such as:\n",
    "\n",
    "r (rank of low-rank matrices)\n",
    "\n",
    "lora_alpha (scaling factor)\n",
    "\n",
    "lora_dropout (dropout rate)\n",
    "\n",
    "task_type (e.g., \"CAUSAL_LM\")\n",
    "\n",
    "## get_peft_model\n",
    "\n",
    "Takes a base model and a LoraConfig, then wraps the model with LoRA layers.\n",
    "\n",
    "The resulting model trains only a small number of new parameters â€” much faster and more memory-efficient.\n",
    "\n",
    "## from datasets import load_dataset\n",
    "\n",
    "Imports the Hugging Face Datasets library function used to load and preprocess datasets easily.\n",
    "\n",
    "load_dataset() automatically downloads datasets from the Hugging Face Hub and returns them in a PyTorch-friendly format.\n",
    "\n",
    "Example:\n",
    "\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "\n",
    "\n",
    "This loads a dataset of English quotes for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41ecbd9b-506c-4d2a-8fa8-dae0dedf8e6d",
   "metadata": {
    "id": "41ecbd9b-506c-4d2a-8fa8-dae0dedf8e6d"
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# LoRA Fine-Tuning and Text Generation with GPT-2\n",
    "# ==========================================\n",
    "# This Jupyter Notebook combines model training and inference into one file.\n",
    "# It fine-tunes GPT-2 using Low-Rank Adaptation (LoRA) on a dataset of English quotes,\n",
    "# then generates text completions using the fine-tuned model.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tpALfzdDOvJw",
   "metadata": {
    "id": "tpALfzdDOvJw"
   },
   "source": [
    "# Imports tools from the PEFT (Parameter-Efficient Fine-Tuning) library.\n",
    "\n",
    "PEFT allows you to fine-tune very large models (like GPT-2) without updating all parameters â€” only small adapter layers (LoRA layers) are trained.\n",
    "\n",
    "#LoraConfig\n",
    "\n",
    "Configuration object that defines LoRA settings such as:\n",
    "\n",
    "r (rank of low-rank matrices)\n",
    "\n",
    "lora_alpha (scaling factor)\n",
    "\n",
    "lora_dropout (dropout rate)\n",
    "\n",
    "task_type (e.g., \"CAUSAL_LM\")\n",
    "\n",
    "#get_peft_model\n",
    "\n",
    "Takes a base model and a LoraConfig, then wraps the model with LoRA layers.\n",
    "\n",
    "The resulting model trains only a small number of new parameters â€” much faster and more memory-efficient.\n",
    "\n",
    "#4. from datasets import load_dataset\n",
    "\n",
    "Imports the Hugging Face Datasets library function used to load and preprocess datasets easily.\n",
    "\n",
    "load_dataset() automatically downloads datasets from the Hugging Face Hub and returns them in a PyTorch-friendly format.\n",
    "\n",
    "Example:\n",
    "\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "\n",
    "\n",
    "This loads a dataset of English quotes for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oPVY22idQ-JB",
   "metadata": {
    "id": "oPVY22idQ-JB"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66a7fb06-151f-4686-9c4f-781a4601447a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66a7fb06-151f-4686-9c4f-781a4601447a",
    "outputId": "77c8329c-1425-47bc-f5af-a73003463835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "# Step2: Setup device (GPU if available), check whether your computer or environment has a GPU that supports CUDA (NVIDIAâ€™s GPU computing platform).\n",
    "\n",
    "# 'cuda' - run computations on the GPU for faster training.\n",
    "\n",
    "# 'cpu' - fallback if GPU is not available.\n",
    "# ------------------------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37905928-b684-4c55-b30d-d8980a3cbb40",
   "metadata": {
    "id": "37905928-b684-4c55-b30d-d8980a3cbb40"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# Step: 3 Load pre-trained GPT-2 model and tokenizer\n",
    "# ------------------------------------------\n",
    "model_name = \"gpt2\" # \"gpt2\" is a model name recognized by Hugging Faceâ€™s Transformers library.\n",
    "\n",
    " # Automatically choose the correct model architecture for causal (left-to-right) language modeling (used for text generation).\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n",
    "\n",
    "#The tokenizer converts text into tokens (numerical IDs) that the model understands, and vice versa (for decoding output back into text).\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# GPT-2 doesn't have a pad token, so we set it to the end-of-sequence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scEpx67zTEpI",
   "metadata": {
    "id": "scEpx67zTEpI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6970619b-8b8b-40c9-843c-efbf1d9b8178",
   "metadata": {
    "id": "6970619b-8b8b-40c9-843c-efbf1d9b8178"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# Step: 4 Freeze base model parameters\n",
    "# ------------------------------------------\n",
    "# This ensures that only LoRA layers will be trained. fine-tuning or LoRA adaptation setups\n",
    "# This line freezes the pretrained GPT-2 model, making sure its existing weights stay unchanged while you train only a small number of new parameters (for example, LoRA layers).\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "643b5655-f2e1-434d-b282-eec2b452d21a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "643b5655-f2e1-434d-b282-eec2b452d21a",
    "outputId": "27cdd586-8580-4d94-cb4e-b55369eb445f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "# Step: 5 Configure and attach LoRA adapter\n",
    "# ------------------------------------------\n",
    "# LoRA adds trainable rank-decomposition matrices to attention layers,\n",
    "# reducing the number of trainable parameters significantly.\n",
    "config = LoraConfig(\n",
    "    r=16,                 # Rank of LoRA matrices\n",
    "    lora_alpha=32,        # Scaling factor, Controls how strongly LoRA updates affect the base model\n",
    "    lora_dropout=0.05,    # Dropout for regularization, Randomly drops 5% of LoRA connections during training to reduce overfitting\n",
    "    bias=\"none\",          # Keeps GPT-2 biases frozen\n",
    "    task_type=\"CAUSAL_LM\" # Language modeling task, Ensures LoRA modifies the right layers for autoregressive text generation\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config) # Makes only those LoRA parameters trainable, while the rest of GPT-2 remains frozen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "406f5e1a-f13d-42d6-b14a-d43e00b7b992",
   "metadata": {
    "id": "406f5e1a-f13d-42d6-b14a-d43e00b7b992"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# Step: 6 Load and preprocess dataset\n",
    "# ------------------------------------------\n",
    "# Using a small public dataset of English quotes from Hugging Face Datasets\n",
    "# Prepares the training dataset so it can be used to fine-tune the GPT-2 model (with LoRA).\n",
    "#creates a single text field that the model can learn from (i.e., input â†’ output structure).\n",
    "# Later, we can have the model predict tags from quotes, or continue learning language style.\n",
    "data = load_dataset(\"Abirate/english_quotes\") # Downloads a public dataset from Hugging Face Hub named \"Abirate/english_quotes\"\n",
    "\n",
    "# Combine 'quote' and 'tags' columns into a single training text\n",
    "def merge_columns(entry):\n",
    "    entry[\"prediction\"] = entry[\"quote\"] + \" ->: \" + str(entry[\"tags\"]) # The new combined text is stored in a new column called \"prediction\"\n",
    "    return entry\n",
    "\n",
    "data['train'] = data['train'].map(merge_columns)\n",
    "\n",
    "# Tokenize the text for model input, This prepares the dataset for training.\n",
    "data = data.map(lambda samples: tokenizer(samples['prediction']), batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50580451-d320-4e12-8287-e0eaad84d2fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50580451-d320-4e12-8287-e0eaad84d2fd",
    "outputId": "da1edd81-6440-4511-edce-ad63b1dbeb7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 589824 | All params: 125029632 | Trainable%: 0.47%\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "# Step 7: Helper function: Display trainable parameter count\n",
    "# ------------------------------------------\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable vs total parameters in the model.\n",
    "    Helps confirm LoRA reduces training load.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"Trainable params: {trainable_params} | All params: {all_param} | \"\n",
    "        f\"Trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c19bc064-24ef-4a07-8581-30ab9fdbe845",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c19bc064-24ef-4a07-8581-30ab9fdbe845",
    "outputId": "f5d9f829-603a-4e82-e099-0a2cef108f05"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "# Step 8: Training setup\n",
    "# this block sets up the training configuration and initializes the trainer that will fine-tune your LoRA-adapted GPT-2 model#\n",
    "# ------------------------------------------\n",
    "# Using the Hugging Face Trainer API for simplicity\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data['train'],\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=4, # Number of examples processed on each GPU (or CPU) before gradient accumulation. Smaller batches reduce memory use.\n",
    "        gradient_accumulation_steps=4, # Accumulates gradients over 4 steps before updating weights â€” effectively making the effective batch size = 4 Ã— 4 = 16.\n",
    "        warmup_steps=100, # Slowly increases the learning rate during the first 100 steps to stabilize training.\n",
    "        max_steps=500, # Stops training after 500 optimization steps (instead of using epochs).\n",
    "        learning_rate=2e-4, # Sets the learning rate for the AdamW optimizer.\n",
    "        logging_steps=50, # Logs training loss every 50 steps.\n",
    "        output_dir='outputs', # Directory where checkpoints, logs, and model outputs will be saved.\n",
    "        auto_find_batch_size=True, # Automatically adjusts the batch size if GPU memory is insufficient.\n",
    "        report_to=\"none\",          # disables W&B and other trackers, Prevents integration with external logging tools (like Weights & Biases or TensorBoard).\n",
    "        logging_dir=None,          # Avoids creating empty log directories when logging is disabled.\n",
    "    ),\n",
    "    # the model learns to predict the next word in a sequence (like GPT-2 normally does), not to fill in masked words (which would be Masked LM like BERT).\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False) # A data collator organizes the dataset samples into batches during training.\n",
    ")\n",
    "\n",
    "# Disable caching during training, caching can cause shape mismatches or extra memory usage â€” so itâ€™s turned off for safety.\n",
    "model.config.use_cache = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "053b32d5-fbc2-4077-a2d0-64a9fa405940",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "053b32d5-fbc2-4077-a2d0-64a9fa405940",
    "outputId": "f16eebb0-80b6-4379-e680-e2e0e5c510bd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 03:16, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.204800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.429200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.983800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.888900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.846000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.853000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.808700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.841400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.816200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.820200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LoRA fine-tuned weights saved as 'lora.pt'\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "# Step 9: Train the LoRA-augmented model\n",
    "# actual training and saving step for your LoRA-adapted GPT-2 model\n",
    "\n",
    "\n",
    "#Calls the Hugging Face Trainer to train the model using your dataset, tokenizer, and training arguments.\n",
    "#What happens under the hood:\n",
    "#Batches are created from data['train'] using the DataCollator.\n",
    "#LoRA parameters in the model are updated via backpropagation.\n",
    "#Gradients accumulate according to gradient_accumulation_steps.\n",
    "#Optimizer updates the trainable parameters (LoRA layers only).\n",
    "#Logging occurs every logging_steps (here, every 50 steps).\n",
    "#Training stops automatically after max_steps=500 or when interrupted.#\n",
    "# ------------------------------------------\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save LoRA fine-tuned weights only (not full GPT-2 model), The base GPT-2 parameters are frozen, so only the small LoRA layers are being trained.\n",
    "# This makes training much faster and lighter on GPU memory.\n",
    "torch.save(model.state_dict(), 'lora.pt')\n",
    "print(\"LoRA fine-tuned weights saved as 'lora.pt'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "382d30e8-d8dd-407b-a821-6821d16c6e20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "382d30e8-d8dd-407b-a821-6821d16c6e20",
    "outputId": "88225f9f-a552-4e2b-abaf-ba0c0f82a652"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Model Output:\n",
      "\n",
      "â€œLife is like a box of chocolates, you never know what you are gonna getâ€ ->: vernacular,chocolates,chocolates-and-chocolates-in-the-world,chocol\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "# Step 10: Inference: Generate text using fine-tuned model\n",
    "# Use the LoRA-adapted GPT-2 for inference (text generation) after training\n",
    "# ------------------------------------------\n",
    "# Re-load LoRA weights to confirm inference works separately from training, Loads the previously saved LoRA weights from disk.\n",
    "model.load_state_dict(torch.load(\"lora.pt\", map_location=device)),\n",
    "model = model.to(device)\n",
    "\n",
    "# Example input prompt for generation\n",
    "prompt = \"â€œLife is like a box of chocolates, you never know what you are gonna getâ€ ->: \"\n",
    "\n",
    "# Tokenize and move to device, Converts the text prompt into token IDs and an attention mask.\n",
    "#return_tensors='pt' returns PyTorch tensors.\n",
    "#.to(device) moves the tensors to the GPU (or CPU) to match the modelâ€™s device.\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate prediction, generates up to 25 new tokens., torch.no_grad disables gradient calculation â€” saves memory and speeds up inference.\n",
    "# Only the LoRA layers affect generation since the base model is frozen.\n",
    "with torch.no_grad():\n",
    "    output_tokens = model.generate(**inputs, max_new_tokens=25)\n",
    "\n",
    "# Decode and print output text, converts token IDs back into human-readable text.\n",
    "print(\"\\n Model Output:\\n\")\n",
    "print(tokenizer.decode(output_tokens[0], skip_special_tokens=True)) #skip_special_tokens=True removes special tokens like `` from the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb65c346-f506-4a80-97c9-8f75295ff59a",
   "metadata": {},
   "source": [
    "## Conclusion: \n",
    "In this project, I have successfully applied Low-Rank Adaptation (LoRA) to fine-tune a pre-trained GPT-2 model for generating text based on English quotes and their associated tags. By freezing the original GPT-2 parameters and training only the small LoRA adapter layers, we achieved parameter-efficient fine-tuning, which significantly reduces computational resources and memory requirements.\n",
    "\n",
    "The workflow demonstrated how to:\n",
    "\n",
    "Prepare and tokenize a dataset for language modeling.\n",
    "\n",
    "Configure LoRA parameters and integrate them into a large language model.\n",
    "\n",
    "Train the model using Hugging Faceâ€™s Trainer with gradient accumulation and proper batching.\n",
    "\n",
    "Save only the fine-tuned LoRA weights, allowing for efficient storage and future reuse.\n",
    "\n",
    "Generate new text using the fine-tuned model, illustrating how the model learned to produce meaningful continuations based on the prompt.\n",
    "\n",
    "Overall, this approach shows that large language models can be effectively adapted to specific tasks without retraining billions of parameters, making it feasible to fine-tune on smaller datasets and limited hardware. LoRA provides a practical, scalable, and resource-efficient solution for customizing pretrained models for specialized applications.\n",
    "\n",
    "While models like ChatGPT-3 and ChatGPT-4 could likely perform quote tagging with few- or zero-shot learning, they are vastly larger than GPT-2 and require massive computational resources. For context, GPT-4 has around 1 trillion parameters, GPT-3 has 175 billion, and GPT-2 has 1.5 billionâ€”already challenging for CPU inference.\n",
    "\n",
    "For lightweight, task-specific applications, fine-tuning a smaller model like GPT-2 is often more practical and efficient, allowing you to run the model locally without relying on overpowered, resource-intensive models like ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c60a50-5dc3-4944-9bc9-7224b8a395bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
