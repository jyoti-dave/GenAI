{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b3d128-e443-458b-ba5e-2f62af82dbc8",
   "metadata": {},
   "source": [
    "# DSC670 - Week9- GAI Production Deployment - MLFLow Observability Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6a73a5-691a-40f8-a838-8d96a6be6bd2",
   "metadata": {},
   "source": [
    "\r",
    "Recreate the MLFlow example outlined in the text and shown in this code.\n",
    "\n",
    "https://github.com/bahree/GenAIBook/blob/main/chapters/ch11/Listing-11.4_mlflow_observability.py\n",
    "\n",
    "This will show how to run the model in a very simple \"app,\" with the MLFlow monitoring running.\r",
    "r own words.ds.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7943b4-8aab-44d7-ba99-8f59f7213f2f",
   "metadata": {},
   "source": [
    "Install some packages to make this work, including:\n",
    "\n",
    "mlflow\n",
    "tiktoken (optional, the author has a simple token counter)\n",
    "prometheus-client\n",
    "\n",
    "Run the command to start MLFLow Server\n",
    "## mlflow server\n",
    "\n",
    "mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts\n",
    "\n",
    "or \n",
    "\n",
    "mlflow server --host 0.0.0.0 --port 5000\n",
    "\n",
    "## Access the url using\n",
    "\n",
    "http://127.0.0.1:5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b045c244-d207-480f-a8ef-136aca39c4e9",
   "metadata": {},
   "source": [
    "## What is MLFlow?\n",
    "MLflow is an open-source platform designed to help manage the complete machine learning (ML) lifecycle. It provides tools that make it easier for data scientists and engineers to track experiments, package code, and deploy models. The main goal of MLflow is to improve collaboration and organization in machine learning projects by making experiments reproducible and models easier to share.\r\n",
    "\r\n",
    "MLflow includes four main components: Tracking, Projects, Models, and Model Registry. The Tracking component helps record and compare model parameters and results. Projects allow users to organize and package code so others can easily run it. Models define a standard way to manage and deploy trained models. The Model Registry serves as a central place to store, version, and manage models throughout their lifecycle.\r\n",
    "\r\n",
    "Overall, MLflow simplifies the process of developing, testing, and deploying machine learning models, ensuring that teams can work together efficiently and maintain consistency across their workflows.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9019266d-660e-47ef-aaff-b71e04031f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets you build simple interactive web apps for your ML/LLM projects\n",
    "#pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5146030b-3bda-4770-b94d-3e6b61e513ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ef4c61e-f4cd-4ad8-9452-99ff6835f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install rquired libraries\n",
    "#mlflow - Logs experiments, metrics, and model runs\n",
    "# tiktoken - Token counting for OpenAI models\n",
    "# colorama - Adds colored output to terminal/Jupyter prints\n",
    "#pip install openai mlflow tiktoken colorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca8a053e-9cd7-4859-b5a8-8fbb0d7079aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/135424484768455482', creation_time=1762621961375, experiment_id='135424484768455482', last_update_time=1762621961375, lifecycle_stage='active', name='GenAI_book', tags={'mlflow.experimentKind': 'genai_development'}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# Generative AI with OpenAI + MLflow\n",
    "# \n",
    "# This notebook demonstrates how to:\n",
    "# - Use the OpenAI API for text generation  \n",
    "# - Log key metrics and parameters with MLflow  \n",
    "# - Count tokens using `tiktoken`  \n",
    "# - Track latency and token usage for each prompt  \n",
    "# - Create a conversational AI loop\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import time\n",
    "import mlflow\n",
    "from openai import OpenAI\n",
    "import tiktoken as tk\n",
    "from colorama import Fore, Style, init\n",
    "## Load required libraries\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#Load variables from .env file into environment\n",
    "load_dotenv()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Configuration and Initialization\n",
    "# - Set your OpenAI API key (from environment variable)\n",
    "# - Configure MLflow tracking URI\n",
    "# - Initialize helper variables\n",
    "\n",
    "# %%\n",
    "# Set OpenAI API key and model parameters\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "MLFLOW_URI = \"http://localhost:5000\" # This is the url where MLFlow will store the data that we log and also assign the name of the experiment.\n",
    "\n",
    "#TEMPERATURE controls how creative or random the model’s responses are.\n",
    "#Low (0.2–0.4): More focused and predictable answers.\n",
    "#High (0.7–1.0): More creative and varied responses.\n",
    "# A value of 0.7 gives a good balance between creativity and accuracy.\n",
    "TEMPERATURE = 0.7\n",
    "\n",
    "# TOP_P is part of nucleus sampling, another randomness control.\n",
    "#TOP_P = 1 means the model considers all possible word choices.\n",
    "#You can lower it (like 0.8) to make the model focus only on the most likely words.\n",
    "TOP_P = 1\n",
    "\n",
    "# FREQUENCY_PENALTY - This reduces how often the model repeats the same phrases.\n",
    "# 0 means no penalty — the model can repeat itself if it decides to.\n",
    "# Increasing this (e.g., 0.5) makes the model avoid repetition.\n",
    "FREQUENCY_PENALTY = 0\n",
    "\n",
    "# This affects how often the model brings up new topics.\n",
    "# 0 means the model can freely continue discussing the same subject.\n",
    "# A higher value encourages it to explore new ideas instead of repeating.\n",
    "PRESENCE_PENALTY = 0\n",
    "\n",
    "#This sets the maximum number of tokens (words + symbols) the model can generate in one response.\n",
    "#Think of tokens as word-pieces — for English, roughly 1 token ≈ 0.75 words.\n",
    "#800 means the model’s response can be up to about 600 words long.\n",
    "MAX_TOKENS = 800\n",
    "\n",
    "#This flag controls debugging mode.\n",
    "#When True, your program will print extra information (like run IDs, token counts, etc.) for troubleshooting.\n",
    "#When False, it stays clean and only shows normal output.\n",
    "DEBUG = False\n",
    "\n",
    "# Initialize colorama\n",
    "init()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "# Set MLflow tracking URI and experiment\n",
    "mlflow.set_tracking_uri(MLFLOW_URI) # MLflow will connect to a tracking server running locally on port 5000 Make sure you’ve already started MLflow by running \"mlflow server\"\n",
    "mlflow.set_experiment(\"GenAI_book\") # Creates a new or switches to that experiment called \"GenAI_book\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b466f301-b036-4193-b251-be5a2508c754",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## Helper Functions\n",
    "# - Colored print for user and AI messages  \n",
    "# - Token counting  \n",
    "# - Text generation function with MLflow logging  \n",
    "def print_user_input(text):\n",
    "    \"\"\"Display user input in green color.\"\"\"\n",
    "    print(f\"{Fore.GREEN}You: {Style.RESET_ALL}\", text)\n",
    "\n",
    "def print_ai_output(text):\n",
    "    \"\"\"Display AI output in blue color.\"\"\"\n",
    "    print(f\"{Fore.BLUE}AI Assistant:{Style.RESET_ALL}\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7eccf35-801a-4cd4-bdc2-6fdc3e61739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return token count, \"cl100k_base\" - default tokenizer \n",
    "def count_tokens(string: str, encoding_name=\"cl100k_base\") -> int:\n",
    "    \"\"\"Count the number of tokens in a given string.\"\"\"\n",
    "    encoding = tk.get_encoding(encoding_name)  # This uses the Tiktoken library (tk) to get a tokenizer (encoding model) by name.\n",
    "    encoded_string = encoding.encode(string, disallowed_special=()) # The disallowed_special=() parameter means \"don't raise errors for special tokens\" like <|endoftext|>).\n",
    "    return len(encoded_string)  # Counts how many tokens were produced — that's the number of elements in the encoded list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ee6e2ae-ae99-4427-bc1d-44d41d86ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(conversation, max_tokens=100) -> str:\n",
    "    \"\"\"Generate text from the OpenAI model and log metrics in MLflow.\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Call OpenAI ChatCompletion API - sends the entire conversation to the model and asks for the next message.\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL, # (e.g., \"gpt-3.5-turbo\"\n",
    "        messages=conversation, #  A list of messages (system, user, assistant).\n",
    "        temperature=TEMPERATURE, # Controls creativity (higher = more random output).\n",
    "        max_tokens=max_tokens, # Limits response length.\n",
    "        top_p=TOP_P, # Controls diversity of output (used with nucleus sampling).\n",
    "        frequency_penalty=FREQUENCY_PENALTY, # Reduces repetition of tokens.\n",
    "        presence_penalty=PRESENCE_PENALTY #Encourages new topics in responses.\n",
    "    )\n",
    "\n",
    "    # Calculate latency\n",
    "    latency = time.time() - start_time # Gives the response time in seconds, useful for performance tracking.\n",
    "    message_response = response.choices[0].message.content # extracts the actual text (content) from the assistant’s message.\n",
    "\n",
    "    # Token counting\n",
    "    prompt_tokens = count_tokens(conversation[-1]['content']) #Number of tokens in the user’s most recent message.\n",
    "    conversation_tokens = count_tokens(str(conversation)) # Tokens in the full chat history.\n",
    "    completion_tokens = count_tokens(message_response) # Tokens generated by the model.\n",
    "\n",
    "    # Log experiment data to MLflow\n",
    "    run = mlflow.active_run() # Gets the currently active MLflow experiment run.\n",
    "    if DEBUG:\n",
    "        print(f\"Run ID: {run.info.run_id}\") # If debug mode is on, it prints the MLflow run ID and pauses execution (useful for troubleshooting).\n",
    "        input(\"Press Enter to continue...\")\n",
    "\n",
    "    # Log metrics, Stores numeric metrics (quantitative data) in MLflow, like latency and token usage for tracking and analysis.\n",
    "    mlflow.log_metrics({\n",
    "        \"request_count\": 1,\n",
    "        \"request_latency\": latency,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"completion_tokens\": completion_tokens,\n",
    "        \"conversation_tokens\": conversation_tokens\n",
    "    })\n",
    "\n",
    "    # Log parameters, Stores static parameters (experiment settings), so we can compare runs later.\n",
    "    mlflow.log_params({\n",
    "        \"model\": MODEL,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"top_p\": TOP_P,\n",
    "        \"frequency_penalty\": FREQUENCY_PENALTY,\n",
    "        \"presence_penalty\": PRESENCE_PENALTY\n",
    "    })\n",
    "    \n",
    "    # Finally, returns the generated text so it can be printed or used elsewhere.\n",
    "    return message_response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb6e3339-4848-4f31-9f3b-15a314001e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/09 11:55:41 INFO mlflow.tracking.fluent: Autologging successfully enabled for openai.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  What is veteran day?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  What is veteran day?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/09 11:56:22 INFO mlflow.tracking.fluent: Autologging successfully enabled for langchain.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Assistant: Veterans Day is a federal holiday in the United States that honors military veterans who have served in the United States Armed Forces. It is observed annually on November 11th, the anniversary of the end of World War I. Veterans Day is a time to recognize and thank all those who have served in the military for their sacrifices and contributions to our country's freedom.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-491ee24ed74b244f4841b6fdbb4d384d&amp;experiment_id=135424484768455482&amp;version=3.5.1\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-491ee24ed74b244f4841b6fdbb4d384d)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ## Run the Conversational Loop\n",
    "# You can run this cell to interact with the model directly.  \n",
    "# Type 'exit', 'quit', or 'q' to stop the loop.\n",
    "\n",
    "# This is the main control loop of your chatbot program, which connects the user, OpenAI API, and MLflow tracking together.\n",
    "\n",
    "#Enable MLflow automatic logging. This tells MLflow to automatically capture: Parameters, Metrics, Model versions, Run metadata\n",
    "# It helps track experiments without manually logging every detail.\n",
    "mlflow.autolog()\n",
    "\n",
    "\n",
    "# This creates a new MLflow run — think of it as one experiment session.\n",
    "# Everything that happens inside this with block (metrics, parameters, logs) is stored in this run.\n",
    "# When the block ends, MLflow automatically closes and saves the run.\n",
    "with mlflow.start_run() as run:\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    ]\n",
    "\n",
    "    #This begins an infinite loop so you can chat continuously with the AI.\n",
    "    while True:\n",
    "        # Take user input\n",
    "        user_input = input(\"User: \") # Waits for the user to type a message.\n",
    "\n",
    "        # Exit condition\n",
    "        # If the user types any of these commands, the loop breaks and the chat ends.\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"q\", \"e\"]: \n",
    "            break\n",
    "\n",
    "        #Show user message : This prints your message in green (from the earlier helper function), just to make the terminal display look cleaner.\n",
    "        print_user_input(user_input)\n",
    "        \n",
    "        #Add user message to the chat: Adds user message to the conversation list so the model has full context for its next response.\n",
    "        conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        #Generate AI response\n",
    "        ai_output = generate_text(conversation, MAX_TOKENS)\n",
    "\n",
    "        #Display AI response\n",
    "        print_ai_output(ai_output)\n",
    "\n",
    "        # Add AI response to conversation\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": ai_output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea3e62d1-ff20-4d79-b6a7-1c2bdf77b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Anaconda prompt app and run below command\n",
    "# mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts\n",
    "# mlflow server\n",
    "# http://127.0.0.1:5000\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
